###########################################
              Computer Vision
###########################################
############################
          Basics
############################
Sampling vs Quantization

Sampling establishes the number of pixels in a digital image, whereas quantization establishes the color of each pixel.
Sampling digitalizes an analog signal's x-axis, whereas quantization digitalizes its y-axis.

############################
   Image Transformations
############################
Morphological transformations

Morphological transformations are some simple operations based on the image shape.
It is normally performed on binary images. It needs two inputs, one is our original image,
second one is called structuring element or kernel which decides the nature of operation.

Erosion, Dilation, Opening (erosion followed by dilation), Closing (dilation followed by erosion),
Morphological Gradient (dilation - erosion), Top Hat, Black Hat

#########
Hit or Miss Transforms

From Wikipedia:
In mathematical morphology, hit-or-miss transform is an operation that detects a given configuration (or pattern)
in a binary image, using the morphological erosion operator and a pair of disjoint structuring elements.

############################
      Edge detection
############################
Laplacian Operator

Laplacian Operator is also a derivative operator which is used to find edges in an image.
The major difference between Laplacian and other operators like Prewitt, Sobel, Robinson and Kirsch
is that these all are first order derivative masks but Laplacian is a second order derivative mask.
|  0  |  1  |  0  |
|  1  | -4  |  1  |
|  0  |  1  |  0  |

or

|  0  | -1  |  0  |
| -1  |  4  | -1  |
|  0  | -1  |  0  |

Laplacian does not provide directions and magnitude of the edges! Just location. On the other hand gradient operators
(first derivative) provides all three features.

###########################################
###########################################
###########################################

https://qr.ae/pvJ1Cs
Why use an HSV image for color detection rather than an RGB image?

The reason we use HSV colorspace for color detection/thresholding over RGB/BGR is that
HSV is more robust towards external lighting changes. This means that in cases of minor
changes in external lighting (such as pale shadows,etc.) Hue values vary relatively
lesser than RGB values.

For example, two shades of red colour might have similar Hue values, but widely
different RGB values. In real life scenarios such as object tracking based on colour,
we need to make sure that our program runs well irrespective of environmental changes
as much as possible. So, we prefer HSV colour thresholding over RGB. :)

###########################################
Image Convolution
https://en.wikipedia.org/wiki/Kernel_(image_processing)
https://programmathically.com/understanding-convolutional-filters-and-convolutional-kernels/
In image processing, a kernel, convolution matrix, or mask is a small matrix used for
blurring, sharpening, embossing, edge detection, and more. This is accomplished by
doing a convolution between the kernel and an image

###########################################
Sobel operator
https://en.wikipedia.org/wiki/Sobel_operator
The Sobel operator, sometimes called the Sobelâ€“Feldman operator or Sobel filter,
is used in image processing and computer vision, particularly within edge detection
algorithms where it creates an image emphasising edges.

###########################################
Canny edge detector
https://docs.opencv.org/4.x/da/d22/tutorial_py_canny.html
Canny Edge Detection is a popular edge detection algorithm. It is a multi-stage
algorithm with the following steps:
1. Noise Reduction
2. Finding Intensity Gradient of the Image (by using sobel operator)
3. Non-maximum Suppression
4. Hysteresis Thresholding

###########################################
Bilateral filter (blur)
https://www.geeksforgeeks.org/python-bilateral-filtering/
A bilateral filter is used for smoothening images and reducing noise, while preserving
edges.


###########################################
                    ML
###########################################
https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/
The pooling operation involves sliding a two-dimensional filter over each channel of
feature map and summarising the features lying within the region covered by the filter.

Why to use Pooling Layers?
+ Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces
the number of parameters to learn and the amount of computation performed in the network.
+ The pooling layer summarises the features present in a region of the feature map
generated by a convolution layer. So, further operations are performed on summarised
features instead of precisely positioned features generated by the convolution layer.
This makes the model more robust to variations in the position of the features in the
input image.

###########################################
ReLU - rectified linear unit
ReLU formula is : f(x) = max(0,x)

https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks
Why is ReLU most commonly used?
One answer:
The main reason why ReLu is used is because it is simple, fast, and empirically it
seems to work well. Empirically, early papers observed that training a deep network
with ReLu tended to converge much more quickly and reliably than training a deep
network with sigmoid activation.

Because there is no difficult arithmetic, the ReLU deep learning function is simple
and does not require any heavy processing. As a result, the model can train or operate
in less time. Sparsity is another significant quality that we consider to be an
advantage of utilizing the ReLU activation function.

###########################################
Image augmentation
Image augmentation is a technique that is used to artificially expand the data-set.
This is helpful when we are given a data-set with very few data samples. In case of
Deep Learning, this situation is bad as the model tends to over-fit when we train it on
limited number of data samples.
###########################################
kaggle.com
Tensors are basically TensorFlow's version of a Numpy array with a few differences that
make them better suited to deep learning. One of the most important is that tensors
are compatible with GPU and TPU) accelerators.

In a well-trained neural network, each layer is a transformation getting us a little
bit closer to a solution.

Without activation functions, neural networks can only learn linear relationships.
In order to fit curves, we'll need to use activation functions.

A common loss function for regression problems is the mean absolute error or MAE.
For each prediction y_pred, MAE measures the disparity from the true target y_true by
an absolute difference abs(y_true - y_pred).

Each iteration's sample of training data is called a minibatch (or often just "batch"),
while a complete round of the training data is called an epoch.

The learning rate and the size of the minibatches are the two parameters that have the
largest effect on how the SGD training proceeds. Their interaction is often subtle and
the right choice for these parameters isn't always obvious.

Adam is an SGD algorithm that has an adaptive learning rate that makes it suitable for
most problems without any parameter tuning (it is "self tuning", in a sense). Adam is
a great general-purpose optimizer.

Underfitting the training set is when the loss is not as low as it could be because the
model hasn't learned enough signal. Overfitting the training set is when the loss is not
as low as it could be because the model learned too much noise. The trick to training
deep learning models is finding the best balance between the two.

You can increase the capacity of a network either by making it wider (more units to
existing layers) or by making it deeper (adding more layers). Wider networks have an
easier time learning more linear relationships, while deeper networks prefer more
nonlinear ones. Which is better just depends on the dataset.

Early stopping - once we detect that the validation loss is starting to rise again,
we can reset the weights back to where the minimum occured. This ensures that the model
won't continue to learn noise and overfit the data.

early_stopping = EarlyStopping(
    min_delta=0.001, # minimium amount of change to count as an improvement
    patience=20, # how many epochs to wait before stopping
    restore_best_weights=True,
)
